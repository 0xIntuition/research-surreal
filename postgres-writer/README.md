# Redis to PostgreSQL Sync Pipeline

A high-performance, event-driven synchronization service that consumes blockchain events from Redis streams (generated by rindexer) and maintains an up-to-date PostgreSQL database with real-time analytics tables.

> **Note**: For a catalog of known issues and potential improvements, see [issues.md](./issues.md).

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Data Flow](#data-flow)
- [Table Hierarchy](#table-hierarchy)
- [Event Processing](#event-processing)
- [Column Calculations](#column-calculations)
- [Configuration](#configuration)
- [Deployment](#deployment)
- [Known Issues and Improvements](#known-issues-and-improvements)
- [Development Notes](#development-notes)

## Overview

This service is the core data synchronization component of the Intuition protocol analytics stack. It:

1. **Consumes blockchain events** from Redis streams in real-time
2. **Maintains event tables** for audit and historical queries
3. **Updates derived tables** using PostgreSQL triggers for simple transformations
4. **Performs complex aggregations** in Rust for multi-level data relationships
5. **Publishes term updates** to Redis for eventual consistency in analytics tables
6. **Runs an analytics worker** that processes term updates and maintains triple-level aggregations

### Key Features

- **High throughput**: Processes 100+ events/second with batching and pipelining
- **Out-of-order handling**: Uses (block_number, log_index) tuples to handle blockchain reorganizations
- **Eventual consistency**: Separates real-time updates (triggers + Rust) from analytics (worker thread)
- **Circuit breaker**: Automatically backs off on failures
- **Observable**: Prometheus metrics and health endpoints
- **Transactional**: All updates within a single event are atomic

## Architecture

### System Design Philosophy

The system evolved from a **materialized view approach** to a **trigger + Rust cascade** approach for better performance and maintainability:

#### Old Approach (migrations 20250130000002-20250130000010)
- Used materialized views for position, vault, term tables
- Required manual or scheduled refreshes (REFRESH MATERIALIZED VIEW)
- Complete recalculation on every refresh
- Slow for incremental updates

#### Current Approach (migrations 20250131000001-20250131000003)
- Event tables remain immutable (audit log)
- PostgreSQL triggers handle simple updates (atom, triple, position, vault)
- Rust cascade processor handles complex aggregations (vault metrics, term totals)
- Analytics worker processes term updates asynchronously for triple-level tables

### Architecture Diagram

```
┌─────────────────┐
│   Rindexer      │  (Blockchain indexer)
│  (Redis Stream) │
└────────┬────────┘
         │ events
         ▼
┌─────────────────────────────────────────────────────────┐
│              EventProcessingPipeline                     │
│  ┌──────────────────────────────────────────────────┐   │
│  │  Worker 1, 2, 3, N (configurable parallelism)   │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────┬───────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────┐
│              PostgreSQL Database                         │
│                                                          │
│  1. Insert into event table                             │
│     ┌──────────────────────────────────────────┐       │
│     │ *_events (atom_created, deposited, etc.) │       │
│     └──────────────┬───────────────────────────┘       │
│                    │ TRIGGER                            │
│                    ▼                                    │
│  2. Trigger updates base table                         │
│     ┌──────────────────────────────────────────┐       │
│     │ atom, triple, position, vault            │       │
│     └──────────────────────────────────────────┘       │
└─────────────────┬───────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────┐
│         CascadeProcessor (Rust, same txn)               │
│                                                          │
│  3. Update vault aggregates from positions              │
│     - position_count                                    │
│     - market_cap calculation                            │
│                                                          │
│  4. Update term aggregates from vaults                  │
│     - total_assets (sum across vaults)                  │
│     - total_market_cap (sum across vaults)              │
│                                                          │
│  5. Transaction commits                                 │
└─────────────────┬───────────────────────────────────────┘
                  │
                  ▼ (after commit)
┌─────────────────────────────────────────────────────────┐
│           RedisPublisher                                 │
│  Publishes term_id to "term_updates" stream             │
└─────────────────┬───────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────┐
│         Analytics Worker (separate thread)               │
│                                                          │
│  Consumes from "term_updates" stream                    │
│                                                          │
│  Updates analytics tables:                              │
│  - triple_vault (pro + counter vault combined)          │
│  - triple_term (aggregate triple_vault)                 │
│  - predicate_object (aggregate by pred-obj pairs)       │
│  - subject_predicate (aggregate by subj-pred pairs)     │
└─────────────────────────────────────────────────────────┘
```

### Components

1. **EventProcessingPipeline** (`src/core/pipeline.rs:16`)
   - Orchestrates worker threads
   - Manages graceful shutdown
   - Monitors health and metrics

2. **RedisStreamConsumer** (`src/consumer/redis_stream.rs`)
   - Consumes events from Redis streams
   - Batches messages for efficiency
   - Acknowledges processed messages

3. **PostgresClient** (`src/sync/postgres_client.rs:12`)
   - Manages database connection pool
   - Processes events and triggers cascade
   - Publishes term updates to Redis

4. **CascadeProcessor** (`src/processors/cascade.rs:19`)
   - Updates vault metrics from positions
   - Updates term metrics from vaults
   - Uses advisory locks to prevent race conditions

5. **Analytics Worker** (`src/analytics/worker.rs:15`)
   - Separate thread consuming term updates
   - Updates triple-level analytics tables
   - Retries failed updates

## Data Flow

### Flow for Deposit/Redeem Events

1. **Event arrives** from Redis stream (e.g., `Deposited`)
2. **Insert into event table** `deposited_events`
3. **Trigger fires** (`trigger_update_position_deposit` in `migrations/20250131000003_create_triggers.sql:212`)
   - Updates `position` table with new shares and cumulative totals
   - Uses `is_event_newer()` to handle out-of-order events
4. **Cascade processor runs** (`CascadeProcessor::process_position_change` in `src/processors/cascade.rs:33`)
   - Acquires advisory lock for position
   - Updates `vault.position_count` by counting active positions
   - Recalculates `vault.market_cap` from current price
   - Aggregates `term.total_assets` and `term.total_market_cap` from all vaults
5. **Transaction commits**
6. **Publish to Redis** `term_updates` stream
7. **Analytics worker** eventually processes update
   - Aggregates triple vault data
   - Updates predicate/object aggregates

### Flow for SharePriceChanged Events

1. **Event arrives** from Redis stream
2. **Insert into event table** `share_price_changed_events`
3. **Trigger fires** (`trigger_update_vault` in `migrations/20250131000003_create_triggers.sql:350`)
   - Updates `vault` table with new price, shares, assets
   - Calculates `market_cap = (total_shares * share_price) / 1e18`
4. **Cascade processor runs** (`CascadeProcessor::process_price_change` in `src/processors/cascade.rs:68`)
   - Acquires advisory lock for vault
   - Recalculates `vault.market_cap`
   - Aggregates `term` totals from all vaults
5. **Transaction commits**
6. **Publish to Redis** and **analytics worker** processes

### Flow for AtomCreated Events

1. **Event arrives** from Redis stream
2. **Insert into event table** `atom_created_events`
3. **Trigger fires** (`trigger_update_atom` in `migrations/20250131000003_create_triggers.sql:79`)
   - Inserts/updates `atom` table
4. **Cascade processor runs** (`CascadeProcessor::process_atom_creation` in `src/processors/cascade.rs:98`)
   - Initializes `term` entry with type='Atom'
5. **Transaction commits**

### Flow for TripleCreated Events

1. **Event arrives** from Redis stream
2. **Insert into event table** `triple_created_events`
3. **Trigger fires** (`trigger_update_triple` in `migrations/20250131000003_create_triggers.sql:134`)
   - Inserts/updates `triple` table
4. **Cascade processor runs** (`CascadeProcessor::process_triple_creation` in `src/processors/cascade.rs:115`)
   - Initializes `term` entries for both pro and counter terms
5. **Transaction commits**

## Table Hierarchy

The database has a clear hierarchy of tables, where higher levels depend on lower levels:

### Level 0: Event Tables (Immutable Audit Log)
- `atom_created_events`
- `triple_created_events`
- `deposited_events`
- `redeemed_events`
- `share_price_changed_events`
- `generic_events`

**Purpose**: Immutable event log for auditing and reprocessing

### Level 1: Base Tables (Updated by Triggers)
- `atom` - Atom definitions (from `atom_created_events`)
- `triple` - Triple relationships (from `triple_created_events`)
- `position` - User positions per (account, term, curve) (from `deposited_events` + `redeemed_events`)
- `vault` - Vault state per (term, curve) (from `share_price_changed_events`)

**Purpose**: Current state of core entities

### Level 2: Aggregated Tables (Updated by Rust Cascade)
- `term` - Aggregated metrics per term across all vaults

**Purpose**: Cross-vault aggregations

### Level 3: Analytics Tables (Updated by Analytics Worker)
- `triple_vault` - Combined pro + counter vault metrics per curve
- `triple_term` - Aggregated triple metrics across all curves
- `predicate_object` - Aggregates by predicate-object pairs
- `subject_predicate` - Aggregates by subject-predicate pairs

**Purpose**: Complex analytics for graph queries

## Event Processing

### Supported Events

| Event | Handler | Trigger | Cascade | Analytics |
|-------|---------|---------|---------|-----------|
| `AtomCreated` | `handle_atom_created` | `update_atom_table` | Initialize term | - |
| `TripleCreated` | `handle_triple_created` | `update_triple_table` | Initialize terms (pro+counter) | - |
| `Deposited` | `handle_deposited` | `update_position_on_deposit` | Update vault & term | Yes |
| `Redeemed` | `handle_redeemed` | `update_position_on_redeem` | Update vault & term | Yes |
| `SharePriceChanged` | `handle_share_price_changed` | `update_vault_on_price_change` | Update term | Yes |

### Event Handler Details

Each event handler (`src/sync/*.rs`):
1. Deserializes event data from JSON
2. Formats addresses using EIP-55 checksum
3. Ensures hex prefixes on IDs
4. Inserts into corresponding event table with `ON CONFLICT` for idempotency
5. Returns, letting triggers and cascade handle the rest

## Transaction Handling and Retry Semantics

### Design Philosophy

The system intentionally uses **separate transactions** for event insertion and cascade updates, combined with idempotent event handlers and Redis retry semantics. This is a deliberate architectural decision that provides better performance and resilience than single-transaction approaches.

### How It Works

```
┌─────────────────────────────────────────────────────────────┐
│  1. Event Processing (Transaction 1)                         │
│     - Insert event into event table                          │
│     - ON CONFLICT DO UPDATE (idempotent)                     │
│     - Database triggers fire                                 │
│     - Update base tables (atom, triple, position, vault)     │
│     ✓ COMMIT                                                 │
└─────────────────────────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────┐
│  2. Cascade Updates (Transaction 2)                          │
│     - Update vault aggregates from positions                 │
│     - Update term aggregates from vaults                     │
│     - Advisory locks prevent race conditions                 │
│     ✓ COMMIT (success) → ACK Redis message                  │
│     ✗ ROLLBACK (failure) → Redis message NOT ack'd          │
└─────────────────────────────────────────────────────────────┘
                         │
                         ▼ (on failure)
┌─────────────────────────────────────────────────────────────┐
│  3. Automatic Retry via Redis                                │
│     - Message redelivered after consumer timeout             │
│     - Event insert hits ON CONFLICT (idempotent)             │
│     - Database triggers fire again (safe)                    │
│     - Cascade retries                                        │
│     ✓ SUCCESS → ACK message                                 │
└─────────────────────────────────────────────────────────────┘
```

### Key Benefits

**1. Better Performance**
- Smaller, faster transactions reduce lock contention
- Event insert and trigger execution complete quickly
- Cascade can take longer without blocking event ingestion
- Higher throughput under load

**2. Resilience Through Idempotency**
- All event handlers use `ON CONFLICT DO UPDATE`
- Safe to retry full operation on cascade failure
- Database triggers are naturally idempotent (update to same values)
- No duplicate data or inconsistent state on retry

**3. Automatic Recovery**
- Redis consumer doesn't ACK messages until full success
- Failed cascades automatically retry via Redis redelivery
- No manual intervention required for transient failures
- Analytics worker provides eventual consistency

### When Could This Fail?

The system is vulnerable only in extreme edge cases:

1. **Process crash between transactions + Redis message loss**
   - Event committed, cascade never runs
   - Message lost before Redis persists it
   - **Mitigation**: Redis persistence (AOF), analytics worker backfill

2. **Database corruption or hardware failure**
   - Would affect any transaction strategy
   - **Mitigation**: Database backups, replication

### Why Not Single Transaction?

A single transaction across event + cascade would:
- ❌ Hold locks longer, reducing throughput
- ❌ Increase deadlock risk on concurrent updates
- ❌ Retry entire operation (event insert + cascade) on any failure
- ❌ Require refactoring all handlers to accept `Transaction<Postgres>`
- ❌ Make triggers and cascade tightly coupled

The separate transaction design provides better performance-safety tradeoff for this use case.

### Related Code

- Event handlers: `src/sync/*.rs` (ON CONFLICT logic)
- Cascade processor: `src/processors/cascade.rs` (transaction 2)
- Redis ACK logic: `src/core/pipeline.rs:279-288`
- Retry semantics: Redis consumer group automatic redelivery

## Column Calculations

### position Table

| Column | Calculation | Updated By |
|--------|-------------|------------|
| `account_id` | Direct from `deposited_events.receiver` or `redeemed_events.receiver` | Trigger |
| `term_id` | Direct from event | Trigger |
| `curve_id` | Direct from event | Trigger |
| `shares` | `total_shares` from latest event (by block_number, log_index) | Trigger |
| `total_deposit_assets_after_total_fees` | Running sum of `deposited_events.assets_after_fees` | Trigger |
| `total_redeem_assets_for_receiver` | Running sum of `redeemed_events.assets` | Trigger |
| `created_at` | Timestamp of first deposit | Trigger |
| `updated_at` | Timestamp of latest event | Trigger |

**Out-of-order handling**: Uses `is_event_newer(new_block, new_log_index, old_block, old_log_index)` helper

### vault Table

| Column | Calculation | Updated By |
|--------|-------------|------------|
| `term_id` | Direct from event | Trigger |
| `curve_id` | Direct from event | Trigger |
| `total_shares` | Direct from `share_price_changed_events.total_shares` | Trigger |
| `current_share_price` | Direct from `share_price_changed_events.share_price` | Trigger |
| `total_assets` | Direct from `share_price_changed_events.total_assets` | Trigger |
| `market_cap` | `(total_shares * current_share_price) / 1e18` | Trigger + Cascade |
| `position_count` | `COUNT(*) FROM position WHERE term_id=X AND curve_id=Y AND shares > 0` | Cascade (`VaultUpdater::update_vault_from_positions` in `src/processors/vault_updater.rs:18`) |
| `vault_type` | Direct from event | Trigger |

### term Table

| Column | Calculation | Updated By |
|--------|-------------|------------|
| `id` | Term ID (either atom or triple) | Cascade |
| `type` | 'Atom', 'Triple', or 'Unknown' | Cascade |
| `atom_id` | term_id if type='Atom', else NULL | Cascade |
| `triple_id` | term_id if type='Triple', else NULL | Cascade |
| `total_assets` | `SUM(vault.total_assets) WHERE vault.term_id = term.id` | Cascade (`TermUpdater::update_term_from_vaults` in `src/processors/term_updater.rs:19`) |
| `total_market_cap` | `SUM(vault.market_cap) WHERE vault.term_id = term.id` | Cascade |

### triple_vault Table

| Column | Calculation | Updated By |
|--------|-------------|------------|
| `term_id` | Triple pro term | Analytics Worker |
| `counter_term_id` | Triple counter term | Analytics Worker |
| `curve_id` | Curve ID | Analytics Worker |
| `total_shares` | `vault(term_id, curve_id).total_shares + vault(counter_term_id, curve_id).total_shares` | Analytics Worker (`update_triple_vault` in `src/analytics/processor.rs:69`) |
| `total_assets` | Sum of pro + counter vault assets | Analytics Worker |
| `position_count` | Sum of pro + counter position counts | Analytics Worker |
| `market_cap` | Sum of pro + counter market caps | Analytics Worker |

### triple_term Table

| Column | Calculation | Updated By |
|--------|-------------|------------|
| `term_id` | Triple pro term | Analytics Worker |
| `counter_term_id` | Triple counter term | Analytics Worker |
| `total_assets` | `SUM(triple_vault.total_assets) WHERE term_id=X AND counter_term_id=Y` | Analytics Worker (`update_triple_term` in `src/analytics/processor.rs:117`) |
| `total_market_cap` | `SUM(triple_vault.market_cap)` | Analytics Worker |
| `total_position_count` | `SUM(triple_vault.position_count)` | Analytics Worker |

### predicate_object Table

| Column | Calculation | Updated By |
|--------|-------------|------------|
| `predicate_id` | Predicate atom ID | Analytics Worker |
| `object_id` | Object atom ID | Analytics Worker |
| `triple_count` | `COUNT(DISTINCT triple.term_id) WHERE predicate_id=X AND object_id=Y` | Analytics Worker (`update_predicate_object` in `src/analytics/processor.rs:157`) |
| `total_position_count` | `SUM(triple_term.total_position_count)` for matching triples | Analytics Worker |
| `total_market_cap` | `SUM(triple_term.total_market_cap)` for matching triples | Analytics Worker |

### subject_predicate Table

| Column | Calculation | Updated By |
|--------|-------------|------------|
| `subject_id` | Subject atom ID | Analytics Worker |
| `predicate_id` | Predicate atom ID | Analytics Worker |
| `triple_count` | `COUNT(DISTINCT triple.term_id) WHERE subject_id=X AND predicate_id=Y` | Analytics Worker (`update_subject_predicate` in `src/analytics/processor.rs:198`) |
| `total_position_count` | `SUM(triple_term.total_position_count)` for matching triples | Analytics Worker |
| `total_market_cap` | `SUM(triple_term.total_market_cap)` for matching triples | Analytics Worker |

## Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `REDIS_URL` | `redis://127.0.0.1:6379` | Redis connection URL |
| `REDIS_STREAMS` | `rindexer_producer` | Comma-separated list of Redis stream names |
| `CONSUMER_GROUP` | `postgres-sync` | Redis consumer group name |
| `CONSUMER_NAME` | `consumer-{uuid}` | Unique consumer name |
| `DATABASE_URL` | **(required)** | PostgreSQL connection URL |
| `BATCH_SIZE` | `100` | Number of messages to process per batch |
| `BATCH_TIMEOUT_MS` | `5000` | Batch timeout in milliseconds |
| `WORKERS` | `4` | Number of parallel worker threads |
| `PROCESSING_TIMEOUT_MS` | `30000` | Event processing timeout |
| `MAX_RETRIES` | `3` | Max retries for failed operations |
| `CIRCUIT_BREAKER_THRESHOLD` | `5` | Failures before circuit opens |
| `CIRCUIT_BREAKER_TIMEOUT_MS` | `60000` | Circuit breaker timeout |
| `HTTP_PORT` | `8080` | HTTP server port for health/metrics |
| `CONSUMER_GROUP_SUFFIX` | *(optional)* | Suffix for analytics worker consumer group |
| `ANALYTICS_STREAM_NAME` | `term_updates` | Redis stream name for term updates (appends suffix if using CONSUMER_GROUP_SUFFIX) |

### Example Configuration

```bash
DATABASE_URL=postgresql://user:pass@localhost:5432/intuition
REDIS_URL=redis://localhost:6379
REDIS_STREAMS=rindexer_producer
WORKERS=8
BATCH_SIZE=100
HTTP_PORT=8080
```

## Deployment

### Building

```bash
cd postgres-writer
cargo build --release
```

### Running

```bash
# Set required environment variables
export DATABASE_URL=postgresql://user:pass@localhost:5432/intuition
export REDIS_URL=redis://localhost:6379

# Run the service
./target/release/postgres-writer
```

### Docker

```bash
docker build -t postgres-writer .
docker run -e DATABASE_URL=... -e REDIS_URL=... postgres-writer
```

### Health Check

```bash
curl http://localhost:8080/health
```

Returns:
```json
{
  "healthy": true,
  "redis_consumer_healthy": true,
  "surreal_sync_healthy": true,
  "circuit_breaker_closed": true,
  "last_check": "2025-01-30T12:00:00Z",
  "metrics": {
    "total_events_processed": 1000,
    "total_events_failed": 0,
    "circuit_breaker_state": "Closed",
    "redis_consumer_health": true,
    "surreal_sync_health": true
  }
}
```

### Metrics

Prometheus metrics available at `http://localhost:8080/metrics`

Key metrics:
- `events_processed_total` - Total events processed
- `events_failed_total` - Total events failed
- `event_processing_duration_seconds` - Processing time histogram
- `circuit_breaker_state` - Current circuit breaker state (0=closed, 1=open)

## Known Issues and Improvements

> **Note**: For a comprehensive catalog of all identified issues, see [issues.md](./issues.md)

### Critical Issues

1. **Health Endpoint Naming Confusion** (See [issues.md#1](./issues.md#1-misleading-health-check-field-names))
   - Health check returns `surreal_sync_healthy` instead of `postgres_sync_healthy`
   - **Impact**: Misleading monitoring and alerting
   - **Status**: Known issue, fix pending

2. **Non-Graceful Shutdown** (See [issues.md#2](./issues.md#2-graceful-shutdown-not-implemented))
   - Analytics worker and HTTP server are aborted on shutdown
   - **Impact**: Potential data loss and duplicate message processing
   - **Mitigation**: Use SIGTERM with grace period for deployment rollouts

### High Priority Issues

3. **Analytics Worker Lag**
   - The analytics worker processes updates asynchronously, so analytics tables may lag behind real-time data
   - **Impact**: Queries on `triple_vault`, `triple_term`, `predicate_object`, `subject_predicate` may be slightly stale
   - **Mitigation**: The lag is typically < 1 second under normal load

4. **Advisory Lock Contention**
   - Heavy concurrent updates to the same position can cause lock contention
   - Uses FNV-1a hash with theoretical collision risk at scale
   - **Impact**: Slight slowdown on high-frequency deposits/redeems; potential race conditions on hash collisions
   - **Mitigation**: Advisory locks are transaction-scoped and release quickly

5. **No Automatic Backfill**
   - If analytics worker fails, updates are retried but there's no automatic backfill mechanism
   - **Impact**: Missed updates require manual intervention
   - **Mitigation**: Failed messages remain in Redis stream (not ACK'd) and will be retried

### Medium Priority Issues

7. **Redis Publisher Lock Contention** (See [issues.md#4](./issues.md#4-redis-publisher-lock-contention))
   - Database queries performed while holding Redis publisher lock
   - **Impact**: Reduced throughput under high load
   - **Mitigation**: Batch size limited to 50 term_ids per batch

8. **Hardcoded Configuration Values**
   - Database connection pool size (10 connections)
   - Analytics rate limits (5000 msg/s)
   - **Impact**: Limited deployment flexibility
   - **Mitigation**: Use environment variables when needed

### Proposed Improvements

1. **Batch Analytics Updates**
   - Currently processes term updates one at a time
   - **Proposal**: Batch multiple term updates into single transactions
   - **Benefit**: 5-10x throughput improvement for analytics worker
   - **Priority**: High

2. **Incremental Triple Analytics**
   - Currently recalculates entire predicate_object and subject_predicate on every update
   - **Proposal**: Maintain incremental counters instead of full aggregations
   - **Benefit**: Faster analytics updates, reduced database load
   - **Priority**: Medium

3. **Unified Transaction Architecture**
   - Refactor to use single transaction for event + cascade
   - **Proposal**: Change all event handlers to accept Transaction<Postgres>
   - **Benefit**: Guaranteed data consistency, simplified error handling
   - **Priority**: Critical (addresses Issue #1)

4. **Event Replay**
   - No built-in mechanism to replay events from scratch
   - **Proposal**: Add command to reset state and replay from Redis stream
   - **Benefit**: Easy recovery from data corruption or schema changes
   - **Priority**: Medium

5. **Enhanced Observability**
   - Currently only basic Prometheus metrics
   - **Proposal**: Add queue depth, processing lag, query performance metrics
   - **Benefit**: Better debugging and capacity planning
   - **Priority**: Medium

6. **Dead Letter Queue**
   - Failed events are retried indefinitely
   - **Proposal**: Add dead letter queue for events that fail after max retries
   - **Benefit**: Better visibility into problematic events
   - **Priority**: Low

## Development Notes

### Migrations Directory

The `migrations/` directory contains both:

1. **Reference implementations** (migrations 20250130000002-20250130000010)
   - Materialized view-based approach
   - **Not used in production**
   - Kept as reference for understanding the original design
   - Documents the transition from materialized views to triggers

2. **Production migrations** (migrations 20250131000001-20250131000003)
   - `20250131000001_drop_materialized_views.sql` - Drops old materialized views
   - `20250131000002_create_regular_tables.sql` - Creates new trigger-based tables
   - `20250131000003_create_triggers.sql` - Creates all triggers for incremental updates

### Code Organization

```
src/
├── main.rs                    # Application entry point
├── lib.rs                     # Library exports
├── config.rs                  # Configuration management
├── error.rs                   # Error types
├── core/
│   ├── pipeline.rs            # Event processing pipeline orchestration
│   ├── circuit_breaker.rs     # Circuit breaker implementation
│   └── types.rs               # Core type definitions
├── consumer/
│   ├── redis_stream.rs        # Redis stream consumer
│   └── redis_publisher.rs     # Redis publisher for analytics
├── sync/
│   ├── postgres_client.rs     # PostgreSQL client and cascade orchestration
│   ├── event_handlers.rs      # Event routing
│   ├── atom_created.rs        # AtomCreated event handler
│   ├── triple_created.rs      # TripleCreated event handler
│   ├── deposited.rs           # Deposited event handler
│   ├── redeemed.rs            # Redeemed event handler
│   ├── share_price_changed.rs # SharePriceChanged event handler
│   └── generic.rs             # Generic event fallback
├── processors/
│   ├── cascade.rs             # Cascade processor orchestration
│   ├── vault_updater.rs       # Vault aggregation logic
│   └── term_updater.rs        # Term aggregation logic
├── analytics/
│   ├── worker.rs              # Analytics worker thread
│   └── processor.rs           # Analytics table update logic
├── monitoring/
│   ├── metrics.rs             # Prometheus metrics
│   └── health.rs              # Health check logic
└── http_server.rs             # HTTP server for health/metrics
```

### Testing

```bash
# Run unit tests
cargo test

# Run with logging
RUST_LOG=debug cargo test

# Build and check
cargo check
cargo clippy
```

### Adding New Event Types

1. Create event struct in `src/sync/your_event.rs`
2. Add handler function following existing patterns
3. Export handler in `src/sync/mod.rs`
4. Add event routing in `src/sync/event_handlers.rs`
5. Add trigger SQL in `migrations/` if needed
6. Add cascade logic in `src/processors/cascade.rs` if needed

---

**License**: MIT OR Apache-2.0

**Authors**: Simonas

**Version**: 0.1.0
