# Redis to PostgreSQL Sync

High-performance Redis Streams to PostgreSQL event synchronization pipeline built with Rust.

## Overview

This service consumes blockchain events from Redis Streams (generated by rindexer) and synchronizes them to PostgreSQL for advanced analytics and querying. It provides:

- **High Performance**: Multi-threaded event processing with configurable batch sizes
- **Reliability**: Circuit breaker pattern, automatic retries, and health checks
- **Monitoring**: Prometheus metrics and health endpoints
- **Type Safety**: Compile-time query verification with sqlx
- **Production Ready**: Comprehensive error handling and logging

## Features

- ✅ **Event-Specific Handlers**: Dedicated handlers for AtomCreated, Deposited, TripleCreated, Redeemed, and SharePriceChanged events
- ✅ **Generic Handler**: Fallback handler for unknown event types using JSONB storage
- ✅ **Idempotent**: Composite primary key (transaction_hash, log_index) prevents duplicates
- ✅ **Schema Migrations**: Automatic database schema setup with sqlx migrations
- ✅ **Connection Pooling**: Efficient database connection management
- ✅ **Consumer Groups**: Distributed processing support via Redis consumer groups

## Architecture

```
Redis Streams → Consumer → Pipeline → Event Handlers → PostgreSQL
      ↓            ↓           ↓             ↓             ↓
  rindexer     XREADGROUP   Workers    Type-specific   Tables
   events     + Consumer   Batch      Deserialization
              Groups      Processing
```

## Database Schema

The service creates the following tables:

- `atom_created_events`: Atom creation events
- `deposited_events`: Deposit events
- `triple_created_events`: Triple (relationship) creation events
- `redeemed_events`: Redemption events
- `share_price_changed_events`: Share price update events
- `generic_events`: Catch-all for unknown event types (JSONB storage)

All tables include:
- Composite primary key: `(transaction_hash, log_index)`
- Transaction metadata: block_number, block_hash, block_timestamp, etc.
- Automatic timestamps: `created_at`
- Indexes on frequently queried fields

## Quick Start

### Prerequisites

- Rust 1.70+ (nightly for optimal build)
- PostgreSQL 12+
- Redis 6.0+

### Configuration

Copy the example environment file and configure:

```bash
cp .env.example .env
```

Edit `.env` with your settings:

```env
# Required
DATABASE_URL=postgresql://user:password@localhost:5432/database
REDIS_URL=redis://localhost:6379

# Optional (defaults shown)
REDIS_STREAMS=rindexer_producer
CONSUMER_GROUP=postgres-sync
BATCH_SIZE=100
WORKERS=4
HTTP_PORT=8080
```

### Running Locally

```bash
# Build
cargo build --release

# Run
cargo run --release
```

### Running with Docker

```bash
# Build image
docker build -t redis-postgres-sync .

# Run container
docker run -d \
  --name redis-postgres-sync \
  -e DATABASE_URL=postgresql://user:password@postgres:5432/database \
  -e REDIS_URL=redis://redis:6379 \
  -p 8080:8080 \
  redis-postgres-sync
```

### Running with Docker Compose

```bash
# Start all services (from research-surreal root)
docker-compose up -d redis-postgres-sync

# View logs
docker-compose logs -f redis-postgres-sync

# Stop
docker-compose down
```

## Configuration Options

### Redis Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `REDIS_URL` | `redis://127.0.0.1:6379` | Redis connection URL |
| `REDIS_STREAMS` | `rindexer_producer` | Comma-separated stream names |
| `CONSUMER_GROUP` | `postgres-sync` | Consumer group name |
| `CONSUMER_NAME` | Auto-generated UUID | Consumer instance name |

### PostgreSQL Settings

| Variable | Required | Description |
|----------|----------|-------------|
| `DATABASE_URL` | Yes | PostgreSQL connection URL |

### Processing Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `BATCH_SIZE` | `100` | Messages per batch |
| `BATCH_TIMEOUT_MS` | `5000` | Batch processing interval |
| `WORKERS` | `4` | Number of worker threads |
| `PROCESSING_TIMEOUT_MS` | `30000` | Processing timeout |
| `MAX_RETRIES` | `3` | Retry attempts |

### Circuit Breaker Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `CIRCUIT_BREAKER_THRESHOLD` | `5` | Failures before opening |
| `CIRCUIT_BREAKER_TIMEOUT_MS` | `60000` | Retry timeout after opening |

### HTTP Server Settings

| Variable | Default | Description |
|----------|---------|-------------|
| `HTTP_PORT` | `8080` | Health/metrics endpoint port |

## Monitoring

### Health Check

```bash
curl http://localhost:8080/health
```

Response:
```json
{
  "healthy": true,
  "redis_consumer_healthy": true,
  "postgres_sync_healthy": true,
  "circuit_breaker_closed": true,
  "last_check": "2025-01-30T12:00:00Z",
  "metrics": {
    "total_events_processed": 12345,
    "total_events_failed": 2,
    "circuit_breaker_state": "closed",
    "redis_consumer_health": true,
    "postgres_sync_health": true
  }
}
```

### Prometheus Metrics

```bash
curl http://localhost:8080/metrics
```

Available metrics:
- `redis_postgres_sync_events_processed_total`: Total events processed
- `redis_postgres_sync_events_failed_total`: Total failed events
- `redis_postgres_sync_batches_processed_total`: Total batches processed
- `redis_postgres_sync_processing_duration_seconds`: Processing duration histogram
- `redis_postgres_sync_events_per_second`: Current processing rate
- `redis_postgres_sync_peak_events_per_second`: Peak processing rate
- `redis_postgres_sync_redis_healthy`: Redis connection health (1=healthy, 0=unhealthy)
- `redis_postgres_sync_postgres_healthy`: PostgreSQL connection health (1=healthy, 0=unhealthy)
- `redis_postgres_sync_uptime_seconds`: Application uptime

## Development

### Running Tests

```bash
cargo test
```

### Database Migrations

Migrations are automatically applied on startup. Manual migration management:

```bash
# Create new migration
sqlx migrate add <migration_name>

# Run migrations
sqlx migrate run --database-url postgresql://user:password@localhost:5432/database

# Revert last migration
sqlx migrate revert --database-url postgresql://user:password@localhost:5432/database
```

### Code Structure

```
redis-postgres-sync/
├── src/
│   ├── main.rs                 # Application entry point
│   ├── lib.rs                  # Library root
│   ├── config.rs               # Configuration management
│   ├── error.rs                # Error types
│   ├── http_server.rs          # Health/metrics endpoints
│   ├── core/                   # Core types and pipeline
│   │   ├── types.rs
│   │   ├── pipeline.rs
│   │   └── circuit_breaker.rs
│   ├── consumer/               # Redis consumer
│   │   └── redis_stream.rs
│   ├── sync/                   # PostgreSQL sync
│   │   ├── postgres_client.rs
│   │   ├── event_handlers.rs
│   │   ├── atom_created.rs
│   │   ├── deposited.rs
│   │   ├── triple_created.rs
│   │   ├── redeemed.rs
│   │   ├── share_price_changed.rs
│   │   ├── generic.rs
│   │   └── utils.rs
│   └── monitoring/             # Metrics and health
│       ├── metrics.rs
│       └── health.rs
├── migrations/                 # Database migrations
│   └── 20250130000001_initial_schema.sql
├── Cargo.toml
├── Dockerfile
└── README.md
```

## Troubleshooting

### Events Not Processing

1. Check Redis connection: `redis-cli -u $REDIS_URL PING`
2. Verify streams exist: `redis-cli XINFO STREAM <stream_name>`
3. Check consumer group: `redis-cli XINFO GROUPS <stream_name>`
4. View health endpoint: `curl http://localhost:8080/health`

### Database Connection Issues

1. Verify connection: `psql $DATABASE_URL -c "SELECT 1"`
2. Check migrations: `sqlx migrate info --database-url $DATABASE_URL`
3. Review logs: `docker-compose logs redis-postgres-sync`

### Performance Tuning

- Increase `WORKERS` for higher throughput
- Adjust `BATCH_SIZE` based on event size
- Lower `BATCH_TIMEOUT_MS` for lower latency
- Monitor with Prometheus/Grafana

## License

MIT OR Apache-2.0

## Author

Simonas
