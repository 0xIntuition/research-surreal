# Migration Plan: Redis Streams ‚Üí RabbitMQ

**Status**: In Progress - Steps 1-7 ‚úÖ Completed, Testing & Monitoring Pending
**Created**: 2025-11-11
**Updated**: 2025-11-11 (after Step 3 completion)
**Goal**: Replace Redis Streams with RabbitMQ for blockchain event distribution and internal messaging

---

## Architecture Decisions

Based on analysis and requirements clarification:

- **Exchange Type**: Direct exchange with routing keys per event type
- **Consumer Pattern**: Separate queues for each service (surreal-writer, postgres-writer)
- **Internal Communication**: Replace `term_updates` Redis stream with Tokio mpsc channel
- **Rindexer Configuration**: Update rindexer.yaml to publish to RabbitMQ

---

## Current Architecture (Redis)

```
Blockchain ‚Üí Rindexer ‚Üí Redis Streams (5 event streams) ‚Üí Dual Consumers
                              ‚Üì
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚Üì                           ‚Üì
         SurrealDB Writer          PostgreSQL Writer
                ‚Üì                           ‚Üì
          SurrealDB                   PostgreSQL
                                            ‚Üì
                                   (publishes to Redis)
                                            ‚Üì
                                   term_updates stream
                                            ‚Üì
                                   Analytics Worker
```

**Redis Streams Used**:
1. `intuition_testnet_atom_created`
2. `intuition_testnet_triple_created`
3. `intuition_testnet_deposited`
4. `intuition_testnet_redeemed`
5. `intuition_testnet_share_price_changed`
6. `term_updates` (internal)

---

## Target Architecture (RabbitMQ)

```
Blockchain ‚Üí Rindexer ‚Üí RabbitMQ Exchanges (5) ‚Üí Queues ‚Üí Dual Consumers
                              ‚Üì
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚Üì                           ‚Üì
         SurrealDB Writer          PostgreSQL Writer
                ‚Üì                           ‚Üì
          SurrealDB                   PostgreSQL
                                            ‚Üì
                                   (mpsc channel)
                                            ‚Üì
                                   Analytics Worker
```

**RabbitMQ Exchanges** (Direct type):
1. `atom_created` ‚Üí routing key: `intuition.atom_created`
2. `triple_created` ‚Üí routing key: `intuition.triple_created`
3. `deposited` ‚Üí routing key: `intuition.deposited`
4. `redeemed` ‚Üí routing key: `intuition.redeemed`
5. `share_price_changed` ‚Üí routing key: `intuition.share_price_changed`

**Queues per Service**:
- SurrealDB Writer: `surreal.atom_created`, `surreal.triple_created`, etc.
- PostgreSQL Writer: `postgres.atom_created`, `postgres.triple_created`, etc.

---

## Migration Steps

### Step 1: Docker Infrastructure Setup ‚≠ê **START HERE**

**Objective**: Add RabbitMQ to Docker Compose and remove Redis

**Files to Modify**:
- `docker/docker-compose.yml`
- `docker/docker-compose.override.yml`

**Changes**:

1. **Add RabbitMQ service** to `docker-compose.override.yml`:
   ```yaml
   rabbitmq:
     image: rabbitmq:3.13-management-alpine
     container_name: rabbitmq
     ports:
       - "18101:5672"      # AMQP protocol
       - "18102:15672"     # Management UI
     environment:
       RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-admin}
       RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASS:-admin}
     volumes:
       - rabbitmq_data:/var/lib/rabbitmq
     networks:
       - intuition-network
     healthcheck:
       test: ["CMD", "rabbitmq-diagnostics", "ping"]
       interval: 10s
       timeout: 5s
       retries: 5
   ```

2. **Add RabbitMQ Prometheus Exporter**:
   ```yaml
   rabbitmq-exporter:
     image: kbudde/rabbitmq-exporter:latest
     container_name: rabbitmq-exporter
     ports:
       - "18401:9419"
     environment:
       RABBIT_URL: http://rabbitmq:15672
       RABBIT_USER: ${RABBITMQ_USER:-admin}
       RABBIT_PASSWORD: ${RABBITMQ_PASS:-admin}
     networks:
       - intuition-network
     depends_on:
       - rabbitmq
   ```

3. **Add volume**:
   ```yaml
   volumes:
     rabbitmq_data:
   ```

4. **Remove Redis services**:
   - Remove `redis` service
   - Remove `redis-commander` service
   - Remove `redis-exporter` service
   - Remove `redis_data` volume

5. **Update environment variables** in service definitions:
   - Replace `REDIS_URL` with `RABBITMQ_URL`
   - Remove `REDIS_STREAMS`, `CONSUMER_GROUP`, `CONSUMER_GROUP_SUFFIX`
   - Add `RABBITMQ_URL=amqp://admin:admin@rabbitmq:5672`

**Port Allocation Changes**:
| Port | Old Service | New Service |
|------|-------------|-------------|
| 18101 | Redis | RabbitMQ (AMQP) |
| 18102 | SurrealDB | RabbitMQ Management UI |
| 18400 | RedisInsight | (removed) |
| 18401 | Redis Exporter | RabbitMQ Exporter |

**Note**: SurrealDB port will need to move to 18103 or similar.

**Validation**:
- [ ] Run `docker compose up -d rabbitmq`
- [ ] Access RabbitMQ Management UI at http://localhost:18102
- [ ] Verify RabbitMQ Prometheus exporter at http://localhost:18401/metrics
- [ ] Confirm RabbitMQ health check passes: `docker inspect rabbitmq`

**Dependencies**: None - this is the first step

**Estimated Effort**: 1-2 hours

---

### Step 2: Rindexer Configuration

**Objective**: Configure rindexer to publish blockchain events to RabbitMQ exchanges

**File to Modify**:
- `@rindexer/rindexer.yaml`

**Changes**:

1. **Add RabbitMQ connection** at the contract level:
   ```yaml
   contracts:
     - name: MultiVault
       details:
         - network: intuition_testnet
           address: "0x2Ece8D4dEdcB9918A398528f3fa4688b1d2CAB91"
           start_block: "8092570"
       abi: "./abis/MultiVault.abi.json"
       include_events:
         - AtomCreated
         - TripleCreated
         - Deposited
         - Redeemed
         - SharePriceChanged
       streams:
         rabbitmq:
           url: ${RABBITMQ_URL}
           exchanges:
             - exchange: atom_created
               exchange_type: direct
               routing_key: intuition.atom_created
               networks:
                 - intuition_testnet
               events:
                 - event_name: AtomCreated

             - exchange: triple_created
               exchange_type: direct
               routing_key: intuition.triple_created
               networks:
                 - intuition_testnet
               events:
                 - event_name: TripleCreated

             - exchange: deposited
               exchange_type: direct
               routing_key: intuition.deposited
               networks:
                 - intuition_testnet
               events:
                 - event_name: Deposited

             - exchange: redeemed
               exchange_type: direct
               routing_key: intuition.redeemed
               networks:
                 - intuition_testnet
               events:
                 - event_name: Redeemed

             - exchange: share_price_changed
               exchange_type: direct
               routing_key: intuition.share_price_changed
               networks:
                 - intuition_testnet
               events:
                 - event_name: SharePriceChanged
   ```

2. **Set environment variable**:
   ```bash
   export RABBITMQ_URL=amqp://admin:admin@localhost:18101
   ```

**Message Format** (rindexer will publish):
```json
{
  "event_name": "AtomCreated",
  "network": "intuition_testnet",
  "contract_address": "0x2Ece8D4dEdcB9918A398528f3fa4688b1d2CAB91",
  "block_number": 8092570,
  "transaction_hash": "0x...",
  "log_index": 0,
  "parameters": {
    "vaultId": "123",
    "creator": "0x...",
    "data": "..."
  }
}
```

**Validation**:
- [ ] Restart rindexer with new configuration
- [ ] Check RabbitMQ Management UI for created exchanges
- [ ] Verify messages appear in exchanges (without consumers yet)

**Dependencies**: Step 1 (RabbitMQ must be running)

**Estimated Effort**: 1 hour

---

### Step 3: SurrealDB Writer Migration ‚úÖ **COMPLETED**

**Objective**: Migrate SurrealDB writer from Redis to RabbitMQ (simpler service, good test case)

**Files Created**:
- `surreal-writer/src/consumer/rabbitmq_consumer.rs` (363 lines)

**Files Modified**:
- `surreal-writer/src/consumer/mod.rs`
- `surreal-writer/src/core/pipeline.rs`
- `surreal-writer/src/core/types.rs` (StreamMessage with optional acker)
- `surreal-writer/src/config.rs`
- `surreal-writer/src/main.rs`
- `surreal-writer/src/error.rs` (removed Redis-specific errors)
- `surreal-writer/src/monitoring/metrics.rs` (renamed Redis ‚Üí RabbitMQ)
- `surreal-writer/Cargo.toml`
- `docker/docker-compose.override.yml` (added RabbitMQ infrastructure)

**Files Deleted**:
- `surreal-writer/src/consumer/redis_stream.rs` (418 lines removed)

**Key Implementation Details**:

1. **Add `lapin` dependency** to `Cargo.toml`:
   ```toml
   [dependencies]
   lapin = "2.3"
   ```

2. **Remove Redis dependency**:
   ```toml
   # Remove: redis = { version = "0.27", features = ["tokio-comp", "streams"] }
   ```

3. **Create RabbitMQ consumer** (`rabbitmq_consumer.rs`):
   - **Multi-Channel Architecture**: Create one channel per queue (5 channels total)
   - Each channel has independent prefetch limit (20 messages)
   - Declare exchanges with `durable: false` (match rindexer settings)
   - Declare queues: `surreal.atom_created`, `surreal.triple_created`, etc. with `durable: true`
   - Bind queues to exchanges with routing keys: `intuition.atom_created`, etc.
   - Implement `basic_consume` with manual acknowledgment
   - **Non-blocking batch collection**: 10ms timeout per message, round-robin across queues
   - **Array event data handling** (CRITICAL):
     - Rindexer can publish multiple blockchain events in a single RabbitMQ message as JSON array
     - Must expand 1 RabbitMQ delivery ‚Üí N StreamMessages
     - Only the last StreamMessage gets the acker
     - RabbitMQ delivery is ACKed only after ALL blockchain events are processed
   - **Poison message rejection**: Malformed JSON ‚Üí nack with requeue=false
   - Implement reconnection logic (Note: exponential backoff not yet implemented)

4. **Update configuration** (`config.rs`):
   ```rust
   pub struct Config {
       pub rabbitmq_url: String,
       pub queue_prefix: String,  // "surreal"
       pub exchanges: Vec<String>, // ["atom_created", "triple_created", ...]
       pub prefetch_count: u16,   // RabbitMQ flow control (20)
       pub batch_size: usize,     // Application batching (100)
       // ... rest of config
   }
   ```

   **Important**: `prefetch_count` and `batch_size` are separate concepts:
   - `prefetch_count`: RabbitMQ QoS setting - max unacked messages per channel
   - `batch_size`: Application-level batching for processing efficiency
   - This separation allows independent tuning of network vs application performance

5. **Update pipeline** (`pipeline.rs`):
   - Replace `RedisStreamConsumer` with `RabbitMQConsumer`
   - Spawn workers per queue instead of per stream
   - Maintain same batch processing logic

**Message Mapping**:
- Redis field `event_data` ‚Üí RabbitMQ message body (JSON)
- Redis message ID ‚Üí RabbitMQ delivery tag (for ack)
- Redis XACK ‚Üí RabbitMQ basic_ack
- Redis consumer group ‚Üí Queue competing consumers

**StreamMessage Structure** (updated in `types.rs`):
```rust
pub struct StreamMessage {
    pub id: String,
    pub event: RindexerEvent,
    pub source_stream: String,
    pub acker: Option<lapin::acker::Acker>,  // Only present on last message in array
}
```

**Array Event Data Handling** (implemented in `rabbitmq_consumer.rs`):
```rust
// Rindexer can publish multiple events in a single message
let event: RindexerEvent = serde_json::from_str(&json_str)?;

if let Some(array) = event.event_data.as_array() {
    let last_index = array.len() - 1;
    for (index, event_data) in array.iter().enumerate() {
        messages.push(StreamMessage {
            id: format!("{queue_name}-{index}"),
            event: individual_event,
            source_stream: queue_name.to_string(),
            // ‚≠ê Only attach acker to the LAST message
            acker: if index == last_index {
                Some(acker.clone())
            } else {
                None
            },
        });
    }
}
```

**Error Handling**:
```rust
// Malformed message (parse error) - don't requeue poison messages
delivery.acker.nack(BasicNackOptions {
    requeue: false,  // Move to DLQ if configured
    multiple: false,
}).await?;

// Processing error (SurrealDB error, transient failures)
rabbitmq_consumer.nack_message(acker, true).await?;  // Requeue for retry
```
- **Poison message detection**: Malformed JSON ‚Üí nack with `requeue: false`
- **Transient errors**: Database errors, timeouts ‚Üí nack with `requeue: true`
- Circuit breaker pattern remains the same
- **Note**: Dead letter queue not explicitly configured (gap from plan)

**Validation**:
- [x] Build: `cargo build` ‚úÖ
- [x] Unit tests: `cargo test` ‚úÖ
- [x] Start service: `docker compose up surreal-writer` ‚úÖ
- [x] Verify messages consumed from RabbitMQ ‚úÖ
- [x] Check SurrealDB for inserted records ‚úÖ
- [x] Monitor metrics at http://localhost:18210/metrics ‚úÖ

**Dependencies**: Steps 1, 2

**Actual Effort**: Successfully completed

**Implementation Commit**: `f6bd743` - 465 additions, 500 deletions across 13 files

---

### Key Patterns from Step 3 to Reuse in Step 4

These patterns were proven during the SurrealDB writer implementation and should be directly applied to the PostgreSQL writer:

#### 1. Array Event Data Handling (CRITICAL!)

**Pattern**:
```rust
fn parse_message(&self, data: &[u8], queue_name: &str, acker: &lapin::acker::Acker)
    -> Result<Vec<StreamMessage>>
{
    let event: RindexerEvent = serde_json::from_str(&json_str)?;

    if let Some(array) = event.event_data.as_array() {
        let last_index = array.len() - 1;
        for (index, event_data) in array.iter().enumerate() {
            messages.push(StreamMessage {
                acker: if index == last_index { Some(acker.clone()) } else { None },
                // ...
            });
        }
    }
}
```

**Why**: Rindexer publishes multiple blockchain events in a single RabbitMQ message. One RabbitMQ delivery can expand to N StreamMessages. Only acknowledge after ALL events are processed.

**Impact**: Critical for correctness - ensures atomic processing of event batches.

---

#### 2. Multi-Channel Architecture

**Pattern**:
```rust
for (exchange, routing_key) in exchanges.iter().zip(routing_keys.iter()) {
    let channel = connection.create_channel().await?;
    channel.basic_qos(prefetch_count, BasicQosOptions::default()).await?;
    let consumer = channel.basic_consume(&queue_name, ...).await?;
    channels.push(Arc::new(Mutex::new(channel)));
    consumers.push(Arc::new(Mutex::new(consumer)));
}
```

**Why**:
- Enables parallel consumption from multiple queues
- Each channel has independent flow control (prefetch)
- No head-of-line blocking between event types

**Impact**: Better throughput and fairness across event types.

---

#### 3. Non-Blocking Batch Collection

**Pattern**:
```rust
for consumer in &self.consumers {
    for _ in 0..batch_size {
        match tokio::time::timeout(Duration::from_millis(10), consumer.next()).await {
            Ok(Some(Ok(delivery))) => { /* process */ },
            Err(_) => break, // Timeout - move to next queue
        }
    }
}
```

**Why**:
- 10ms timeout prevents starvation when one queue is empty
- Round-robin across all queues up to batch_size
- Low latency, responsive batch collection

**Impact**: Fair processing across queues, no blocking on empty queues.

---

#### 4. Batch-Level Acknowledgment

**Pattern**:
```rust
for message in messages {
    let result = process_event(&message.event).await;

    if let Some(ref acker) = message.acker {
        match result {
            Ok(_) => acker.ack(BasicAckOptions::default()).await?,
            Err(_) => acker.nack(BasicNackOptions { requeue: true, ... }).await?,
        }
    }
}
```

**Why**:
- One RabbitMQ message = atomic unit of work
- If any event fails, entire batch is redelivered
- Guarantees at-least-once delivery semantics

**Impact**: Critical for correctness with array event data.

---

#### 5. Poison Message Rejection

**Pattern**:
```rust
match serde_json::from_str::<RindexerEvent>(&json_str) {
    Ok(event) => { /* process */ },
    Err(e) => {
        // Malformed message - don't requeue
        delivery.acker.nack(BasicNackOptions {
            requeue: false,
            multiple: false,
        }).await?;
    }
}
```

**Why**:
- Prevents infinite retry loops on unparseable messages
- Moves to dead letter queue (if configured)
- Protects queue from being blocked by poison messages

**Impact**: Operational safety and reliability.

---

#### 6. Separation of Prefetch and Batch Size

**Pattern**:
```rust
pub struct Config {
    pub prefetch_count: u16,   // RabbitMQ flow control (20)
    pub batch_size: usize,     // Application batching (100)
}
```

**Why**:
- `prefetch_count` controls memory usage and network flow
- `batch_size` controls application processing efficiency
- Independent tuning of network vs application performance

**Impact**: Flexible performance tuning.

---

### Step 4: PostgreSQL Writer Migration

**Objective**: Migrate PostgreSQL writer from Redis to RabbitMQ for event consumption AND analytics worker

**Status**: ‚úÖ **COMPLETED** (with architectural refinement)

**Strategy**:
1. **Event Consumption**: Same as Step 3 - consume from RabbitMQ queues instead of Redis streams ‚úÖ
2. **Analytics Worker**: ‚úÖ **Uses Tokio mpsc channel instead of RabbitMQ** (architectural decision)
   - postgres-writer sends term updates via in-process `tokio::sync::mpsc::unbounded_channel<String>`
   - Analytics worker consumes from mpsc receiver
   - **Rationale for mpsc over RabbitMQ**: Term updates are internal communication within the same service. Using mpsc provides:
     - Simpler architecture (no unnecessary RabbitMQ serialization/deserialization)
     - Faster communication (in-memory, no network overhead)
     - Type-safe (no JSON parsing errors)
     - Lower latency for analytics updates
     - Eliminates the RabbitMQ publisher lock contention issue that was documented in PostgresClient

**Files Created**:
- `postgres-writer/src/consumer/rabbitmq_consumer.rs` ‚úÖ (copied and adapted from surreal-writer)

**Files Modified**:
- `postgres-writer/src/consumer/mod.rs` ‚úÖ
- `postgres-writer/src/consumer/rabbitmq_publisher.rs` ‚úÖ (simplified to only TermUpdateMessage struct)
- `postgres-writer/src/core/pipeline.rs` ‚úÖ (accepts optional mpsc sender)
- `postgres-writer/src/core/types.rs` ‚úÖ (StreamMessage with optional acker)
- `postgres-writer/src/sync/postgres_client.rs` ‚úÖ (uses mpsc sender instead of RabbitMQ publisher)
- `postgres-writer/src/analytics/worker.rs` ‚úÖ (consumes from mpsc channel with batching)
- `postgres-writer/src/config.rs` ‚úÖ (removed analytics RabbitMQ settings)
- `postgres-writer/src/main.rs` ‚úÖ (creates mpsc channel and wires components)
- `postgres-writer/Cargo.toml` ‚úÖ
- `docker/docker-compose.override.yml` ‚úÖ (postgres-writer service config)

**Files Deleted**:
- `postgres-writer/src/consumer/redis_stream.rs` ‚úÖ
- `postgres-writer/src/consumer/redis_publisher.rs` ‚úÖ

**Key Implementation Details**:

**IMPORTANT**: Apply all patterns from "Key Patterns from Step 3" section above, especially:
- Array event data handling (critical!)
- Multi-channel architecture
- Non-blocking batch collection with 10ms timeout
- Batch-level acknowledgment
- Poison message rejection

1. **Add dependencies** to `Cargo.toml`:
   ```toml
   [dependencies]
   lapin = "2.3"  # RabbitMQ client (same as surreal-writer)
   # Note: tokio mpsc not needed - analytics worker uses RabbitMQ queue instead
   ```

2. **Create RabbitMQ consumer** (`rabbitmq_consumer.rs`):
   - **Copy implementation from surreal-writer** and adapt queue prefix
   - Multi-channel architecture (one channel per queue)
   - Queues: `postgres.atom_created`, `postgres.triple_created`, etc.
   - Array event data handling (1 RabbitMQ delivery ‚Üí N StreamMessages)
   - Non-blocking batch collection (10ms timeout)
   - Poison message rejection (malformed JSON ‚Üí requeue=false)

3. **Term update communication** (mpsc channel):
   ```rust
   // In main.rs - create channel
   let (term_update_tx, term_update_rx) = tokio::sync::mpsc::unbounded_channel::<String>();

   // Pass sender to PostgresClient
   let postgres_client = PostgresClient::new(
       &config.database_url,
       config.database_pool_size as u32,
       Some(term_update_tx),  // ‚Üê mpsc sender
       metrics.clone(),
   ).await?;

   // Pass receiver to analytics worker
   analytics::start_analytics_worker(
       config,
       pool,
       metrics,
       term_update_rx,  // ‚Üê mpsc receiver
       cancellation_token,
   ).await?;
   ```

4. **Update PostgresClient** (`sync/postgres_client.rs`):
   ```rust
   pub struct PostgresClient {
       pool: PgPool,
       cascade_processor: CascadeProcessor,
       // Tokio mpsc sender for term updates (lock-free, in-process communication)
       term_update_tx: Option<tokio::sync::mpsc::UnboundedSender<String>>,
       metrics: Arc<Metrics>,
   }

   // After cascade commit, send term updates
   if let Some(ref tx) = self.term_update_tx {
       for term_id in &term_ids {
           if let Err(e) = tx.send(term_id.clone()) {
               warn!("Failed to send term update via mpsc: {}", e);
           }
       }
   }
   ```

5. **Update analytics worker** (`analytics/worker.rs`):
   ```rust
   pub async fn start_analytics_worker(
       _config: Config,
       pool: PgPool,
       metrics: Arc<Metrics>,
       mut term_update_rx: UnboundedReceiver<String>,
       cancellation_token: CancellationToken,
   ) -> Result<()> {
       loop {
           // Collect batch of term_ids from mpsc channel
           let term_ids = collect_batch(&mut term_update_rx, 100, 100ms).await?;

           // Process each term update
           for term_id in term_ids {
               let term_update = TermUpdateMessage {
                   term_id: term_id.clone(),
                   timestamp: Utc::now().timestamp(),
               };

               update_analytics_tables(&pool, &metrics, &term_update).await?;
           }
       }
   }
   ```

6. **Update configuration** (`config.rs`):
   ```rust
   pub struct Config {
       // RabbitMQ settings (for blockchain event consumption)
       pub rabbitmq_url: String,
       pub exchanges: Vec<String>,  // Event exchanges
       pub queue_prefix: String,    // "postgres"
       pub prefetch_count: u16,     // 20
       pub batch_size: usize,       // 100

       // Note: Analytics settings removed - now uses in-process mpsc channel
       // Removed: analytics_exchange, analytics_routing_key, max_messages_per_second, min_batch_interval_ms

       // ... rest of config (PostgreSQL, processing, circuit breaker, HTTP)
   }
   ```

   Removed: `REDIS_URL`, `REDIS_STREAMS`, `CONSUMER_GROUP`, `ANALYTICS_STREAM_NAME`, `ANALYTICS_EXCHANGE`, `ANALYTICS_ROUTING_KEY`, `MAX_MESSAGES_PER_SECOND`, `MIN_BATCH_INTERVAL_MS`
   Added: `RABBITMQ_URL`, `QUEUE_PREFIX`, `EXCHANGES`

**Benefits of mpsc Channel over RabbitMQ for Term Updates**:
- **Simpler architecture**: No serialization/deserialization overhead
- **Faster communication**: In-memory, no network latency
- **Type-safe**: No JSON parsing errors (compile-time type checking)
- **Lock-free**: Eliminates the RabbitMQ publisher lock contention bottleneck
- **Lower resource usage**: No additional RabbitMQ connections/channels
- **Appropriate scope**: Term updates are internal communication within a single service

---

#### PostgreSQL-Specific Considerations

**Error Classification and Requeue Strategy**:

```rust
// Error categories for requeue decisions
match error {
    // PERMANENT ERRORS - Do NOT requeue (requeue=false)
    Error::Parse(_) => {
        // Malformed JSON - poison message
        nack(requeue: false)
    }
    Error::Database(ref db_err) if is_constraint_violation(db_err) => {
        // Duplicate key, FK violation - won't succeed on retry
        nack(requeue: false)
    }

    // TRANSIENT ERRORS - Requeue for retry (requeue=true)
    Error::Database(ref db_err) if is_connection_error(db_err) => {
        // Connection lost, timeout, deadlock
        nack(requeue: true)
    }
    Error::CircuitBreakerOpen => {
        // Temporary circuit breaker - retry later
        nack(requeue: true)
    }

    // Default: Requeue transient errors
    _ => nack(requeue: true)
}

fn is_constraint_violation(err: &sqlx::Error) -> bool {
    matches!(err, sqlx::Error::Database(db_err) if
        db_err.code() == Some("23505") || // unique_violation
        db_err.code() == Some("23503")    // foreign_key_violation
    )
}

fn is_connection_error(err: &sqlx::Error) -> bool {
    matches!(err,
        sqlx::Error::PoolTimedOut |
        sqlx::Error::PoolClosed |
        sqlx::Error::Io(_) |
        sqlx::Error::Database(db_err) if db_err.code() == Some("40P01") // deadlock_detected
    )
}
```

**Acknowledgment Strategy with Transactions**:

The postgres-writer uses **separate transactions** for event insertion and cascade updates:

```rust
async fn process_event(
    message: &StreamMessage,
    postgres_client: &PostgresClient,
    publisher: &RabbitMQPublisher,
) -> Result<()> {
    // Transaction 1: Insert event (idempotent via ON CONFLICT)
    postgres_client.insert_event(&message.event).await?;
    // Triggers fire here, updating base tables

    // Transaction 2: Cascade updates (vault metrics, term totals)
    postgres_client.process_cascade(&message.event, publisher).await?;

    // ACK only after BOTH transactions succeed
    if let Some(ref acker) = message.acker {
        acker.ack(BasicAckOptions::default()).await?;
    }

    Ok(())
}
```

**Why this works safely**:
1. **Event insert is idempotent** (`ON CONFLICT DO UPDATE`)
2. If cascade fails, message is nack'd ‚Üí RabbitMQ redelivers
3. On redelivery, event insert succeeds (duplicate) ‚Üí cascade retries
4. Triggers use `is_event_newer()` to handle out-of-order events
5. Advisory locks prevent race conditions in cascade processor

**Ordering Guarantees**:
- RabbitMQ queues are FIFO within a single consumer
- Multiple workers may process messages out of order
- **Existing safeguard**: `is_event_newer()` helper in triggers handles this
- No changes needed to out-of-order event handling

---

**Validation**:
- [x] Build: `cargo build` ‚úÖ
- [x] Start service: `docker compose up postgres-writer --build` ‚úÖ
- [x] Verify messages consumed from RabbitMQ (event queues) ‚úÖ
- [x] Check PostgreSQL for inserted records ‚úÖ
- [x] Verify analytics worker starts with mpsc channel ‚úÖ
- [x] Verify NO parse errors for term updates ‚úÖ
- [x] Monitor metrics at http://localhost:18211/metrics ‚úÖ
- [x] Verify term updates flow through mpsc channel correctly ‚úÖ

**Dependencies**: Steps 1, 2, 3

**Actual Effort**: ~3 hours (faster than estimated due to mpsc simplification)

**Key Achievement**: Eliminated the parse error by using mpsc channels instead of RabbitMQ for term updates, which was simpler and more appropriate for internal communication

---

### Step 5: Testing Infrastructure Update

**Objective**: Update integration tests to use RabbitMQ testcontainers

**Files to Modify**:
- `postgres-writer/tests/helpers/test_harness.rs`
- `postgres-writer/tests/*.rs` (all integration tests)
- `postgres-writer/Cargo.toml` (dev dependencies)

**Key Changes**:

1. **Update dev dependencies** in `Cargo.toml`:
   ```toml
   [dev-dependencies]
   # Remove: testcontainers-modules = { version = "0.11", features = ["redis"] }
   # Add:
   testcontainers-modules = { version = "0.11", features = ["rabbitmq"] }
   ```

2. **Update test harness** (`test_harness.rs`):
   ```rust
   use testcontainers_modules::rabbitmq::RabbitMq;

   pub struct TestHarness {
       pub postgres: PostgresContainer,
       pub rabbitmq: RabbitMqContainer,
       pub rabbitmq_url: String,
       pub database_url: String,
       // Remove: redis, redis_url
   }

   impl TestHarness {
       pub async fn new() -> Self {
           let rabbitmq = RabbitMq::default().start().await;
           let rabbitmq_url = format!(
               "amqp://guest:guest@localhost:{}",
               rabbitmq.get_host_port_ipv4(5672).await
           );

           // ... rest of setup
       }

       pub async fn publish_message(
           &self,
           exchange: &str,
           routing_key: &str,
           message: &Event,
       ) -> Result<()> {
           let conn = lapin::Connection::connect(&self.rabbitmq_url, Default::default()).await?;
           let channel = conn.create_channel().await?;

           // Declare exchange
           channel.exchange_declare(
               exchange,
               lapin::ExchangeKind::Direct,
               Default::default(),
               Default::default(),
           ).await?;

           // Publish message
           let payload = serde_json::to_vec(&message)?;
           channel.basic_publish(
               exchange,
               routing_key,
               Default::default(),
               &payload,
               Default::default(),
           ).await?;

           Ok(())
       }
   }
   ```

3. **Update test cases**:
   - Replace `publish_to_stream()` with `publish_message()`
   - Replace stream names with exchange names
   - Adjust assertions for RabbitMQ semantics

**Validation**:
- [ ] Run tests: `cargo test`
- [ ] All integration tests pass
- [ ] Verify test isolation (each test gets fresh RabbitMQ)

**Dependencies**: Step 4

**Estimated Effort**: 3-4 hours

---

### Step 6: Monitoring & Metrics Update

**Objective**: Update Prometheus metrics and Grafana dashboards for RabbitMQ

**Files to Modify**:
- `postgres-writer/src/monitoring/metrics.rs`
- `surreal-writer/src/monitoring/metrics.rs`
- `infrastructure/monitoring/prometheus/prometheus.yml`
- `infrastructure/monitoring/grafana/dashboards/*.json`

**Key Changes**:

1. **Update Prometheus scrape configs** (`prometheus.yml`):
   ```yaml
   scrape_configs:
     # Remove redis-exporter job

     # Add RabbitMQ exporter
     - job_name: 'rabbitmq'
       static_configs:
         - targets: ['rabbitmq-exporter:9419']
   ```

2. **Update application metrics** (`metrics.rs`):
   ```rust
   // Remove Redis-specific metrics:
   // - stream_lag
   // - messages_claimed
   // - last_message_timestamp

   // Add RabbitMQ-specific metrics:
   lazy_static! {
       pub static ref QUEUE_DEPTH: IntGaugeVec = register_int_gauge_vec!(
           "rabbitmq_queue_depth",
           "Number of messages in queue",
           &["queue"]
       ).unwrap();

       pub static ref MESSAGES_CONSUMED: IntCounterVec = register_int_counter_vec!(
           "rabbitmq_messages_consumed_total",
           "Total messages consumed from queue",
           &["queue"]
       ).unwrap();

       pub static ref MESSAGES_ACKED: IntCounterVec = register_int_counter_vec!(
           "rabbitmq_messages_acked_total",
           "Total messages acknowledged",
           &["queue"]
       ).unwrap();

       pub static ref MESSAGES_NACKED: IntCounterVec = register_int_counter_vec!(
           "rabbitmq_messages_nacked_total",
           "Total messages negatively acknowledged",
           &["queue"]
       ).unwrap();
   }
   ```

3. **Update Grafana dashboards**:
   - Replace Redis panels with RabbitMQ panels
   - Use RabbitMQ exporter metrics
   - Update queries to use new metric names

**New Dashboard Panels**:
- Queue depth per queue
- Message rates (consumed/acked/nacked)
- Consumer count per queue
- Message age in queues
- Connection status

**Validation**:
- [ ] Prometheus scrapes RabbitMQ exporter
- [ ] Application metrics appear in Prometheus
- [ ] Grafana dashboards display data
- [ ] No Redis metrics remain

**Dependencies**: Step 4

**Estimated Effort**: 2-3 hours

---

### Step 7: Documentation Update ‚úÖ **COMPLETED**

**Objective**: Update all documentation to reflect RabbitMQ architecture

**Files to Modify**:
- `README.md`
- `CLAUDE.md`
- `postgres-writer/README.md`
- `docker/README.md` (if exists)

**Key Updates**:

1. **Update architecture diagrams** - Replace Redis with RabbitMQ
2. **Update port allocation table** - RabbitMQ ports
3. **Update environment variables** - Remove Redis, add RabbitMQ
4. **Update development commands** - RabbitMQ management UI
5. **Update message flow documentation** - RabbitMQ semantics
6. **Add RabbitMQ setup instructions**

**New Sections to Add**:

**RabbitMQ Management**:
```markdown
### RabbitMQ Management

Access RabbitMQ Management UI at http://localhost:18102
- Username: admin
- Password: admin

Useful management tasks:
- View queues and message counts
- Monitor consumer connections
- Check exchange bindings
- Purge queues for testing
```

**Files Updated**:
- `README.md` - Architecture, data flow, port allocation, environment variables
- `CLAUDE.md` - Architecture, configuration, technology stack
- `postgres-writer/README.md` - Architecture diagram, event flow, configuration

**Key Changes Made**:
1. ‚úÖ Updated architecture diagrams - Replaced Redis with RabbitMQ exchanges and queues
2. ‚úÖ Updated port allocation tables - RabbitMQ ports (18101 AMQP, 18102 Management, 18401 Exporter)
3. ‚úÖ Updated environment variables - Removed Redis, added RabbitMQ (RABBITMQ_URL, EXCHANGES, QUEUE_PREFIX, PREFETCH_COUNT)
4. ‚úÖ Updated data flow documentation - RabbitMQ exchanges, routing keys, queue patterns
5. ‚úÖ Added RabbitMQ Management UI instructions (http://localhost:18102, admin/admin)
6. ‚úÖ Updated internal communication - Documented mpsc channels for term updates
7. ‚úÖ Updated technology stack - lapin 2.3, RabbitMQ 3.13

**Validation**:
- [x] All documentation updated
- [x] Redis references replaced with RabbitMQ (except migration history and testcontainer references)
- [x] Architecture diagrams reflect RabbitMQ
- [x] Port allocation reflects new layout
- [x] Environment variables documented correctly
- [x] mpsc channel usage for term updates documented

**Dependencies**: Step 6

**Actual Effort**: ~1 hour (completed 2025-11-11)

---

## Rollback Plan

If migration fails or issues arise:

1. **Keep Redis configuration** in a separate branch/tag
2. **Docker**: Revert docker-compose files to include Redis
3. **Code**: Git revert to pre-migration commit
4. **Data**: No data loss (databases unchanged)
5. **Rindexer**: Revert rindexer.yaml configuration

**Safety Measures**:
- Test thoroughly on local development first
- Deploy to staging before production
- Keep Redis infrastructure running in parallel initially
- Monitor error rates and performance metrics closely

---

## Success Criteria

- [x] RabbitMQ running and accessible via management UI ‚úÖ
- [x] Rindexer successfully publishes to RabbitMQ exchanges ‚úÖ
- [x] SurrealDB writer consumes events and writes to SurrealDB ‚úÖ
- [x] PostgreSQL writer consumes events and writes to PostgreSQL ‚úÖ
- [x] Analytics worker receives term updates via mpsc channel ‚úÖ
- [ ] All integration tests pass (Step 5 pending)
- [x] No Redis dependencies remain in code (only in testcontainers) ‚úÖ
- [ ] Monitoring/metrics working with RabbitMQ (Step 6 pending)
- [x] Documentation fully updated ‚úÖ
- [ ] Performance equal to or better than Redis implementation (monitoring needed)

---

## Timeline Estimate

**Total Effort**: ~15 hours actual (vs 22-34 hours estimated)

| Step | Effort | Status | Notes |
|------|--------|--------|-------|
| 1. Docker Infrastructure | 1-2h | ‚úÖ Completed | Foundation |
| 2. Rindexer Config | 1h | ‚úÖ Completed | Depends on 1 |
| 3. SurrealDB Writer | 4-6h | ‚úÖ Completed | Test case first |
| 4. PostgreSQL Writer | ~3h | ‚úÖ Completed | Simpler with mpsc channels for term updates |
| 5. Testing | 3-4h | ‚è≥ Next | Update integration tests for RabbitMQ |
| 6. Monitoring | 2-3h | ‚è≥ Pending | Update Prometheus/Grafana configs |
| 7. Documentation | ~1h | ‚úÖ **COMPLETED** | All docs updated for RabbitMQ |

**Steps 1-4, 7 Completed**: Full migration to RabbitMQ complete!
- Docker infrastructure ‚úÖ
- Rindexer configuration ‚úÖ
- SurrealDB Writer ‚úÖ
- PostgreSQL Writer ‚úÖ (with mpsc channels for analytics)
- Documentation ‚úÖ (README, CLAUDE.md, postgres-writer/README.md)

**Recommended Approach**: Complete steps sequentially, testing thoroughly after each step.

---

## Notes

**Implementation Status**:
- ‚úÖ Steps 1-4, 7 completed successfully
- üîÑ Step 5 (Testing Infrastructure) is next priority
- ‚è≥ Step 6 (Monitoring & Metrics) can be done in parallel

**Key Discoveries**:

**From Step 3 (SurrealDB Writer)**:
- **Array event data handling** is critical - rindexer can publish multiple blockchain events in a single RabbitMQ message
- Multi-channel architecture (one channel per queue) works well for parallel consumption
- Non-blocking batch collection with 10ms timeout prevents starvation
- Poison message detection (malformed JSON ‚Üí requeue=false) provides operational safety
- Prefetch count (20) and batch size (100) should be kept separate for independent tuning
- See "Key Patterns from Step 3" section for detailed patterns to reuse

**From Step 4 (PostgreSQL Writer)**:
- **Architectural Refinement**: Switched from RabbitMQ to Tokio mpsc channels for term updates
- **Problem Solved**: Eliminated the parse error (`missing field 'event_name'`) by using proper communication pattern
- **Rationale**: Term updates are internal communication within a single service, not external message bus traffic
- **Benefits**: Simpler code, better performance, type-safe, lock-free, appropriate scope
- **Pattern**: External events (blockchain) ‚Üí RabbitMQ, Internal events (term updates) ‚Üí mpsc

**Migration Strategy**:
- Each step should be tested before proceeding to next
- SurrealDB writer served as simpler test case before PostgreSQL writer
- Maintain idempotency and retry semantics throughout migration
- RabbitMQ's automatic redelivery is simpler than Redis XCLAIM pattern
- **Use appropriate messaging layer**: RabbitMQ for external events, mpsc for internal communication

---

## Questions / Decisions Needed

- [ ] RabbitMQ version: 3.13 (latest stable) - confirmed
- [ ] Should we use RabbitMQ quorum queues for better durability?
- [ ] Dead letter queue strategy for failed messages?
- [ ] Message TTL configuration?
- [ ] Max queue length limits?
- [ ] Should exchanges be durable or transient?


